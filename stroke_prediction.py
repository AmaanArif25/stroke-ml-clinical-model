# -*- coding: utf-8 -*-
"""Stroke Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zPHRrVzNYPX0LqWH6ZmoR53ojupMEGRg
"""

import pandas as pd

try:
    df = pd.read_csv('healthcare-dataset-stroke-data.csv')
    display(df.head())
    print(df.shape)
except FileNotFoundError:
    print("Error: 'healthcare-dataset-stroke-data.csv' not found. Please ensure the file exists in the current directory or provide the correct path.")
except Exception as e:
    print(f"An error occurred: {e}")

# Identify missing values
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100
print("Missing Values:\n", missing_values)
print("\nMissing Value Percentage:\n", missing_percentage)

# Examine feature distributions for numerical features
numerical_features = ['age', 'avg_glucose_level', 'bmi']
print("\nDescriptive Statistics for Numerical Features:")
display(df[numerical_features].describe())

# Examine feature distributions for categorical features
categorical_features = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status', 'stroke']
for col in categorical_features:
    print(f"\nValue counts for {col}:\n{df[col].value_counts()}")

# Data type consistency
print("\nData Types:")
print(df.dtypes)

# Potential inconsistencies
print("\nPotential Inconsistencies:")
print("1. Negative or unrealistic values in 'age', 'avg_glucose_level', 'bmi'.")
print("2. 'bmi' has missing values, which needs to be imputed.")
print("3. 'smoking_status' has 'Unknown' values, which might need further investigation or imputation.")
print("4. Check for unexpected combinations of features.")

import numpy as np

# Impute missing 'bmi' values with the median
df['bmi'].fillna(df['bmi'].median(), inplace=True)

# Winsorize outliers in 'age', 'avg_glucose_level', and 'bmi'
from scipy.stats.mstats import winsorize
df['age'] = winsorize(df['age'], limits=[0.05, 0.05])
df['avg_glucose_level'] = winsorize(df['avg_glucose_level'], limits=[0.05, 0.05])
df['bmi'] = winsorize(df['bmi'], limits=[0.05, 0.05])

# Handle 'Other' in 'gender' (replace with 'Male' for simplicity)
df['gender'].replace('Other', 'Male', inplace=True)

# Handle 'Unknown' in smoking_status (create a separate category)
df['smoking_status'].replace('Unknown', 'Not Available', inplace=True)


# One-hot encode categorical features
categorical_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Verify data types (optional, but good practice)
print(df.dtypes)

# Save the cleaned data
df.to_csv('cleaned_healthcare_data.csv', index=False)

import pandas as pd
import numpy as np
from scipy.stats.mstats import winsorize

# Create a copy of the original DataFrame to work with
df_cleaned = df.copy()

print(df_cleaned.columns)

# Impute missing 'bmi' values with the median
df_cleaned['bmi'] = df_cleaned['bmi'].fillna(df_cleaned['bmi'].median())

# Winsorize outliers in 'age', 'avg_glucose_level', and 'bmi'
df_cleaned['age'] = winsorize(df_cleaned['age'], limits=[0.05, 0.05])
df_cleaned['avg_glucose_level'] = winsorize(df_cleaned['avg_glucose_level'], limits=[0.05, 0.05])
df_cleaned['bmi'] = winsorize(df_cleaned['bmi'], limits=[0.05, 0.05])

# The one-hot encoding has already been done

# Verify data types
print(df_cleaned.dtypes)

# Save the cleaned data
df_cleaned.to_csv('cleaned_healthcare_data.csv', index=False)

# Create interaction features
df_cleaned['age_glucose_interaction'] = df_cleaned['age'] * df_cleaned['avg_glucose_level']
df_cleaned['hypertension_heart_disease'] = df_cleaned['hypertension'] * df_cleaned['heart_disease']

# Transform existing features
df_cleaned['log_avg_glucose_level'] = np.log1p(df_cleaned['avg_glucose_level'])
df_cleaned['bmi_squared'] = df_cleaned['bmi'] ** 2

# Save the DataFrame with the new features
df_cleaned.to_csv('engineered_features.csv', index=False)

from sklearn.model_selection import train_test_split

# Separate features (X) and target variable (y)
X = df_cleaned.drop('stroke', axis=1)
y = df_cleaned['stroke']

# Split data into training (70%), validation (15%), and testing (15%) sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Save the splits to CSV files
X_train.to_csv('X_train.csv', index=False)
y_train.to_csv('y_train.csv', index=False)
X_val.to_csv('X_val.csv', index=False)
y_val.to_csv('y_val.csv', index=False)
X_test.to_csv('X_test.csv', index=False)
y_test.to_csv('y_test.csv', index=False)

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
import time

# Load the training data
X_train = pd.read_csv('X_train.csv')
y_train = pd.read_csv('y_train.csv')

# Initialize the models
model_lr = LogisticRegression(max_iter=1000)  # Increased max_iter to avoid convergence warnings
model_rf = RandomForestClassifier()
model_svc = SVC()
model_gb = GradientBoostingClassifier()

# Train the models and measure training time
models = {
    'Logistic Regression': model_lr,
    'Random Forest': model_rf,
    'Support Vector Machine': model_svc,
    'Gradient Boosting': model_gb
}

for model_name, model in models.items():
    start_time = time.time()
    model.fit(X_train, y_train.values.ravel())
    end_time = time.time()
    training_time = end_time - start_time
    print(f"{model_name} training time: {training_time:.2f} seconds")

import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
import joblib

# Load the validation data
X_val = pd.read_csv('X_val.csv')
y_val = pd.read_csv('y_val.csv')

# Define parameter grids for each model
param_grid_lr = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}
param_grid_rf = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]}
param_grid_svc = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
param_grid_gb = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}

# Initialize models
model_lr = LogisticRegression(max_iter=1000)
model_rf = RandomForestClassifier()
model_svc = SVC()
model_gb = GradientBoostingClassifier()

# Perform GridSearchCV for each model
models = {
    'Logistic Regression': (model_lr, param_grid_lr),
    'Random Forest': (model_rf, param_grid_rf),
    'Support Vector Machine': (model_svc, param_grid_svc),
    'Gradient Boosting': (model_gb, param_grid_gb),
}

best_estimators = {}
for model_name, (model, param_grid) in models.items():
    grid_search = GridSearchCV(model, param_grid, scoring='f1', cv=5)
    grid_search.fit(X_val, y_val.values.ravel())
    best_estimators[model_name] = grid_search.best_estimator_
    print(f"Best hyperparameters for {model_name}: {grid_search.best_params_}")

# Save best estimators
for model_name, best_estimator in best_estimators.items():
    joblib.dump(best_estimator, f'{model_name}_best_estimator.pkl')

import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import joblib

# Load the test data
X_test = pd.read_csv('X_test.csv')
y_test = pd.read_csv('y_test.csv')

# Load the saved models
model_names = ['Logistic Regression', 'Random Forest', 'Support Vector Machine', 'Gradient Boosting']
loaded_models = {}
for model_name in model_names:
    try:
        loaded_models[model_name] = joblib.load(f'{model_name}_best_estimator.pkl')
    except FileNotFoundError:
        print(f"Warning: Model file '{model_name}_best_estimator.pkl' not found. Skipping this model.")
        continue

# Evaluate the models
results = []
for model_name, model in loaded_models.items():
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    try:  # Handle potential errors for AUC-ROC calculation
        auc_roc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])
    except AttributeError:
        print(f"Warning: AUC-ROC could not be calculated for {model_name}.")
        auc_roc = "N/A"
    results.append([model_name, accuracy, precision, recall, f1, auc_roc])

# Display results in a table
results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC-ROC'])
display(results_df)

# Analyze and identify best performing model
# (Further analysis can be added here based on the obtained metrics)

